{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![INSA](https://gi.insa-lyon.fr/sites/all/themes/insa_satellites/logo.png)\n",
    "\n",
    "# GI-5-DSC - Data Science: Clustering\n",
    "***\n",
    "\n",
    "The objective of this part of the tutorial is to continue the analysis of velo'v data and to experiment with artificial intelligence methods for data clustering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up the environment: import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import folium\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import geopandas\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "import sklearn.cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting the data\n",
    "\n",
    "First, the dataset must be loaded. \n",
    "On the one hand the data set containing the locations of the stations and on the other hand the usage history. \n",
    "The second one has been modified during the previous session. In order not to have to redo all the processing you can retrieve the `data-bikes-2.zip` archive directly.\n",
    "\n",
    "\n",
    "All the data used in this tutorial is available on the [git repository](https://github.com/ludovicmoncla/insa-5gi-dsc-tutorials/tree/main/data) and on [Moodle](https://moodle.insa-lyon.fr/course/view.php?id=4628). \n",
    "\n",
    "\n",
    "* Download the datasets\n",
    "1. data-stations.zip\n",
    "2. data-bikes-2.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Loading the data\n",
    "\n",
    "As last time, to load the data you just have to use the method [read_csv()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#pandas.read_csv) from the `Pandas` library. \n",
    "It takes as a parameter the path of the file you want to load. This file can be of 2 formats, either directly a CSV file, or a ZIP file containing a CSV. In our case it is therefore unnecessary to unzip the previously downloaded archives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## We load the data from the stations into a dataframe\n",
    "df_stations = pd.read_csv('data/data-stations.zip')\n",
    "\n",
    "## We now load the dataframe with the history data\n",
    "df_bikes = pd.read_csv('data/data-bikes-2.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the first rows\n",
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the first rows\n",
    "df_bikes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the size in memory\n",
    "df_bikes['time'] = pd.to_datetime(df_bikes['time']) \n",
    "df_bikes[['year', 'daily_departure', 'daily_arrival']] = df_bikes[['year', 'daily_departure', 'daily_arrival']].astype('int16')\n",
    "df_bikes[['month','day','hour','minute', 'bikes', 'bike_stands', 'departure30min','arrival30min']] = df_bikes[['month','day','hour','minute', 'bikes', 'bike_stands', 'departure30min','arrival30min']].astype('int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering\n",
    "\n",
    "Our objective in this part is to identify groups of \"similar\" stations. To do so, we will apply unsupervised learning methods: clustering.\n",
    "The objective is not to group the stations by spatial proximity but by a similarity calculated from the historical data of the use of the stations (departures and arrivals).\n",
    "\n",
    "### 3.1 Preparing the data\n",
    "\n",
    "First we have to make sure that all stations are comparable. We are therefore interested in knowing if they all have the same amount of data. The lack of data can be due to bugs in the data collection process but it can also be due to stations that have closed during the year.\n",
    "\n",
    "In order to check if all stations have the same amount of data, we can group the dataframe rows according to the station name and then display the size of each group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We group the dataframe rows by station\n",
    "g = *****\n",
    "\n",
    "# Display the size of each group\n",
    "print(g.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all stations are displayed but we can already see that one station has less data than the others. We need to determine how many stations are different to remove them.\n",
    "Use the plot() function to display a graph of the size (number of data) of each station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the size of each group on a graph\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that 6 stations have clearly less data than the others. For the rest of the processing we will remove these 6 stations from the dataset.\n",
    "\n",
    "Identify the name of these 6 stations by using the functions count() and [nsmallest()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.nsmallest.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of the 6 stations with the least data is displayed\n",
    "list_to_drop = *****\n",
    "list_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check the size of the dataframe before deleting the lines of the concerned stations\n",
    "df_bikes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the 6 stations from the df_bikes dataframe\n",
    "\n",
    "df_bikes.drop(df_bikes.loc[*****].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check the size of the dataframe after deletion\n",
    "df_bikes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to group the stations by similarity, we will add some variables (columns). In particular, we will be interested in the number of departures (and arrivals) normalized by day of the week. \n",
    "\n",
    "1. Use the method [to_datetime()](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html) to transform the column `time` type.\n",
    "2. Then create a new column `day_of_week` using the method [day_name()](https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.day_name.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a new column with the name of the day of the week\n",
    "df_bikes['day_of_week'] = df_bikes['time'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bikes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to add columns with the mean daily values of departures and arrivals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We calculate the average arrivals and departures (daily) per station and per day of the week\n",
    "arrivals = *****\n",
    "departures = *****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrivals = arrivals.unstack(level=1) # transform rows into columns\n",
    "arrivals = arrivals.fillna(0) # we replace the empty null values\n",
    "arrivals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departures = departures.unstack(level=1) # transform rows into columns\n",
    "departures = departures.fillna(0) # we replace the empty null values\n",
    "departures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We combine these two datasets into a single one that will serve as a training set for the clustering algorithm\n",
    "\n",
    "df_data = departures.merge(arrivals, how='inner', on=['id_velov'])\n",
    "df_data = df_data.fillna(0)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check if there are infinite values (not compatible with the clustering algo)\n",
    "np.any(np.isfinite(df_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We replace these values by the value 1\n",
    "df_data.replace([np.inf, -np.inf], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training and use of the clustering model\n",
    "\n",
    "Most of machine learning methods are already implemented in the [Scikit-Learn](https://scikit-learn.org/stable/) library. This library includes a large number of unsupervised (clustering) and supervised (classification and regression) learning algorithms.\n",
    "\n",
    "\n",
    "The [Kmeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) algorithm is a widely used clustering algorithm. Its principle is simple: group the data into k homogeneous and compact clusters. In order to create homogeneous clusters the algorithm is based on a distance calculation between the data and the centroid of the different clusters. These centroids are recalculated each time a new data is added to the cluster. \n",
    "\n",
    "Our goal here is to determine if the stations can be grouped into 2 distinct clusters based on the similarity of their usage history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model declaration\n",
    "model = *****\n",
    "\n",
    "# training the model\n",
    "*****\n",
    "\n",
    "# use of the model to associate a cluster number to each row of the dataset\n",
    "df_data[\"cluster\"] = *****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to display these clusters on a map we need to add the latitude/longitude coordinates of the stations to our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = *****\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Cluster mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_stations = *****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The geodataframe data is displayed directly on a map \n",
    "## with the scatter_mapbox() method of the plotly.express library:\n",
    "fig = *****\n",
    "\n",
    "## We remove the margins around the map\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.update(layout_coloraxis_showscale=False) # remove the colorbar\n",
    "\n",
    "## Display the map\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the 2 clusters are also geographically distinct. One cluster is located in the center and the second one is located in the periphery."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dsc-5gi-py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c1a0333fc97fefd4ce94a1c8087d31dccee1782a1c25c68ad4f13412d9023b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
