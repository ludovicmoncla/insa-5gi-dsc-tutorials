{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![INSA](https://gi.insa-lyon.fr/sites/all/themes/insa_satellites/logo.png)\n",
    "\n",
    "# GI-5-DSC - Data Science: Introduction to data exploration & visualization\n",
    "***\n",
    "\n",
    "\n",
    "The purpose of this tutorial is to familiarize ourselves with data exploration and visualization. \n",
    "We will start using Python libraries to manipulate data.\n",
    "For that, we will work on a case study which consists in the analysis of the data of the Lyon Metropole bike sharing platform (Vélo'v). \n",
    "\n",
    "The data we are going to use come from the [Grand Lyon open data platform](https://data.grandlyon.com). They are made available free of charge by Lyon Metropole and can be downloaded in different formats: https://data.grandlyon.com/jeux-de-donnees/historique-disponibilites-stations-velo-v-metropole-lyon/donnees.\n",
    "\n",
    "From the Grand Lyon website, only the last 7 days are available.\n",
    "\n",
    "\n",
    "![site du grand lyon](https://perso.liris.cnrs.fr/lmoncla/GI-5-DSC/fig/grandlyon.png)\n",
    "\n",
    "\n",
    "In this tutorial, you will have at your disposal the entire dataset for the year 2021.\n",
    "\n",
    "\n",
    "The objectives of this tutorial are: \n",
    "\n",
    "* Load the dataset into a dataframe\n",
    "* Explore and visualize the data\n",
    "* Analyze the data: query the dataset to generate graphs and maps\n",
    "\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up the environment: import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import folium\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting the data\n",
    "\n",
    "To overcome the limit of 7 days of availability on the Grand Lyon website, I have developed a script that automatically retrieves and stores the data every day. You will thus have access to the data for the whole year 2021. \n",
    "I also propose the data in a CSV format (easier to load into a dataframe than the original JSON format). \n",
    "\n",
    "\n",
    "If you cloned the [github repository](https://github.com/ludovicmoncla/insa-5gi-dsc-tutorials) you already have the data and you can go directly to section 2.2.\n",
    "\n",
    "\n",
    "### 2.1 Download the data\n",
    "\n",
    "All the data used in this tutorial are available on the `data` folder of the [github repository](https://github.com/ludovicmoncla/insa-5gi-dsc-tutorials/tree/main/data).\n",
    "\n",
    "* Download the two zip files containing the data:\n",
    "1. data-stations.zip\n",
    "2. data-bikes.zip\n",
    "\n",
    "These 2 zip files contain a CSV file containing respectively the list of velov stations (and their location) and the list of availabilities of each station by 30 minutes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Load the data\n",
    "\n",
    "\n",
    "In this tutorial we will not use a DBMS. The objective is to load the data in memory into a Python structure and to query it directly. \n",
    "\n",
    "There are two types of data:\n",
    "1. vélo'v stations (station id, latitude, longitude),\n",
    "2. their history (station id, year, month, day, hour, minute, date, bikes available, available slots).\n",
    "\n",
    "To manipulate these data we will use the  [dataframes](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) of the Pandas library.\n",
    "\n",
    "Pandas is a Python library specialized in data analysis and manipulation. It provides in particular an object of type 'dataframe' which makes it possible to carry out operations of preprocessing and filtering which we will use to request the data.\n",
    "\n",
    "The first objectives are the following:\n",
    "\n",
    "1. To store in a first dataframe the list of velo'v stations and their associated latitude / longitude coordinates.\n",
    "2. To store in a second dataframe for each station and each timestamp the following data: \n",
    "    * id of the station\n",
    "    * year\n",
    "    * month\n",
    "    * day\n",
    "    * hour\n",
    "    * minute\n",
    "    * complete date (original format)\n",
    "    * number of available bikes\n",
    "    * number of free slots\n",
    "    * number of departures in the last 30 minutes\n",
    "    * number of arrivals in the last 30 minutes\n",
    "\n",
    "To load the data you just have to use the method [read_csv()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#pandas.read_csv) of the `Pandas` library. It takes as a parameter the path of the file you want to load. This file can be of 2 formats, either directly a CSV file, or a ZIP file containing a CSV. In our case, it is useless to unzip the previously downloaded archives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Load the data from the stations into a dataframe\n",
    "df_stations = pd.read_csv('data/data-stations.zip')\n",
    "\n",
    "## Create the dataframe with the history data\n",
    "df_bikes = pd.read_csv('data/data-bikes.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the type of our variable\n",
    "type(df_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the list of columns\n",
    "df_stations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print the five first rows\n",
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How many velo'v stations are there ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show the size of the dataframe\n",
    "## The shape method returns the dimensions (rows / columns)\n",
    "print(df_stations.shape)\n",
    "\n",
    "## The len() function returns the number of rows\n",
    "print(len(df_stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show the first rows\n",
    "df_bikes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. First look at the history data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display some information about the data\n",
    "df_bikes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduction of the size in memory\n",
    "## we transform the type of the columns into integer or float when it is necessary\n",
    "df_bikes.bikes = df_bikes.bikes.apply(lambda x: int(float(x)))\n",
    "df_bikes.bike_stands = df_bikes.bike_stands.apply(lambda x: np.int32(float(x)))\n",
    "\n",
    "df_bikes['year'] = df_bikes['year'].astype('int16')\n",
    "df_bikes[['month','day','hour','minute', 'bikes', 'bike_stands', 'departure30min','arrival30min']] = df_bikes[['month','day','hour','minute', 'bikes', 'bike_stands', 'departure30min','arrival30min']].astype('int8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display some information about the data\n",
    "df_bikes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display some stats about the dataframe\n",
    "df_bikes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show 5 rows randomly selected\n",
    "df_bikes.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Handling of a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get data from one column\n",
    "df_bikes['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get data from one column (other method using .)\n",
    "df_bikes.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get data from a list of columns\n",
    "df_bikes[['time', 'bikes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the value from a set of column as a numpy array\n",
    "df_bikes[['time', 'bikes']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A column (or variable) is a data vector (Series in the Pandas library terminology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the first values of a specific column\n",
    "df_bikes['time'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the last values of a specific column\n",
    "df_bikes['time'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sort the values from a specific column\n",
    "df_bikes['time'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can also sort an entire dataframe\n",
    "## Sort the dataframe using the station id and the time\n",
    "df_bikes.sort_values(by=['id_velov', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternative to reset the row indexes to zero\n",
    "df_bikes = df_bikes.sort_values(by=['id_velov', 'time']).reset_index(drop=True)\n",
    "df_bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the number of rows for each distinct value in a specific column\n",
    "df_bikes['id_velov'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A column being a vector it is possible to use indices to access the elements\n",
    "## Display of the first value of the time column\n",
    "df_bikes['time'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display of the first 3 values of the time column\n",
    "df_bikes['time'][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Iterate over columns\n",
    "\n",
    "Iterations on variables can be done via a loop, or via the use of callback functions called with an `.apply()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop over all columns to display their name and type\n",
    "for col in df_bikes.columns:\n",
    "    print(col, \": \", df_bikes[col].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Iterate over rows (\\*\\*not recommended for large dataframes**)\n",
    "\n",
    "\n",
    "It is possible to browse the rows of a dataframe, but beware, iteration on a dataframe is slow. It is better to use vector operations! If you can't, you should use a callback function called with an `.apply()` function.\n",
    "\n",
    "Note: you cannot modify a dataframe on which you are looping.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For the example, we iterate on the stations dataframe (because the history one is too big)\n",
    "for index, row in df_stations.iterrows():\n",
    "    print('ID :', row.id_velov, '\\t lat :', row.latitude,'\\t lng :', row.longitude)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Indexed access to the data of a DataFrame\n",
    "\n",
    "The values of the DataFrame can be accessed via indices or ranges of indices. \n",
    "The structure then behaves like a matrix. The top left cell has coordinates (0,0).\n",
    "There are different ways to do this, using `.iloc[,]` is one of the simplest solutions.\n",
    "\n",
    "Reminder: the `shape()` method is used to get the dimensions (rows and columns) of the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Access to the value located in (0,0) (first row, first column)\n",
    "df_bikes.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Value located in last row, first column\n",
    "## Use of negative index\n",
    "df_bikes.iloc[-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternative with shape, value located in last row, first column\n",
    "## shape[0] returns the number of lines (first dimension)\n",
    "## you have to reduce by -1 because the first index is equal to 0 otherwise you overflow\n",
    "df_bikes.iloc[df_bikes.shape[0]-1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the first 5 values of all columns\n",
    "## rows => 0:5 (0 à 5 [not included])\n",
    "## columns = : (all columns)\n",
    "df_bikes.iloc[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## With the negative marking, we can easily access the last 5 rows\n",
    "df_bikes.iloc[-5:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5 first rows and columns 0, 6, 7 and 8\n",
    "## we have a list of indices for the columns\n",
    "df_bikes.iloc[0:5,[0,6,7,8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 Filtering with conditions - Queries\n",
    "\n",
    "We can isolate subsets of observations that meet criteria defined on columns. We will preferentially use the `.loc[,]` method in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of history data for station 'velov-10001'\n",
    "df_bikes.loc[df_bikes['id_velov']==\"velov-10001\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For a set of values of the same variable we use the isin() method\n",
    "df_bikes.loc[df_bikes['id_velov'].isin(['velov-10001','velov-10002']),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logical operators are used to combine conditions.\n",
    "We use respectively: & for AND, | for OR, and ~ for negation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of data for station 'velov-10001' and hour = 8\n",
    "df_bikes.loc[(df_bikes['id_velov']==\"velov-10001\") & (df_bikes['hour'] == 8),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of data from after July\n",
    "df_bikes.loc[(df_bikes['month'] > 7),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can display only part of the columns\n",
    "# We define the projection in a list\n",
    "colonnes = ['id_velov','time','bikes','bike_stands']\n",
    "# that we use as parameter in .loc[]\n",
    "# example with the same restriction as before\n",
    "df_bikes.loc[(df_bikes['month'] > 7),colonnes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5 Grouping of rows\n",
    "\n",
    "The use of `groupby()` allows access to the sub-DataFrame associated with each item of the grouping variable. It is then possible to explicitly apply further processing on these subsets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping data according to the station id\n",
    "g = df_bikes.groupby('id_velov')\n",
    "\n",
    "g.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the dimension of the sub-DataFrame associated with the station 'velov_10001'\n",
    "g.get_group('velov-10001').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Visualization of station locations\n",
    "\n",
    "Now that you loaded the data in dataframes and see how to manipulate dataframes, you'll produce your first data viz with these data. As we have geographic data we will display them on a map.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 Use of GeoPandas and Plotly libraries\n",
    "\n",
    "The[GeoPandas](https://geopandas.org/) library is developped to manipulate spatial data. The particularity of GeoPandas is that it allows to manipulate spatial data as if it were traditional data.. \n",
    "\n",
    "Compared to a standard `DataFrame`, a `GeoDataFrame` has an additional column: `geometry`. As in a spatial DBMS, this column allows to store the contours (the geometry) of a geographic object. A `GeoDataFrame` object inherits the properties of a `DataFrame` but offers methods adapted to the processing of spatial data.\n",
    "\n",
    "Thus, in addition to the manipulations already possible with pandas, it will be possible to manipulate the spatial dimension: \n",
    "- calculate distances and surfaces,\n",
    "- quickly aggregate areas (grouping departments into regions for example),\n",
    "- search for an area from the coordinates of a point,\n",
    "- convert data into different projection systems,\n",
    "- display a map.\n",
    "\n",
    "For the moment we are working on the last item to produce a map of the Velo'v stations.\n",
    "\n",
    "![stations velov avec GeoPandas](https://perso.liris.cnrs.fr/lmoncla/GI-5-DSC/fig/geopandas_stations.png)\n",
    "\n",
    "* Display the vélo'v stations on a map. Use the [GeoPandas](https://geopandas.org/gallery/create_geopandas_from_pandas.html#sphx-glr-gallery-create-geopandas-from-pandas-py) library. You should get the above result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We transform the dataframe of stations into a geodataframe (https://geopandas.org/gallery/create_geopandas_from_pandas.html#sphx-glr-gallery-create-geopandas-from-pandas-py)\n",
    "gdf_stations = geopandas.GeoDataFrame(\n",
    "    df_stations, \n",
    "    geometry=geopandas.points_from_xy(df_stations.longitude, df_stations.latitude))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the first rows of the geodataframe to check the existence of the geometry column\n",
    "gdf_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the geodataframe on a map using the scatter_mapbox() method from plotly.express\n",
    "fig = px.scatter_mapbox(gdf_stations,\n",
    "                        lat=gdf_stations.geometry.y,\n",
    "                        lon=gdf_stations.geometry.x,\n",
    "                        hover_name=\"id_velov\",\n",
    "                        zoom=12, mapbox_style=\"carto-positron\")\n",
    "\n",
    "## We remove margin around the map\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "\n",
    "## Show the map\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preprocessing\n",
    "\n",
    "### 3.1. Adding information\n",
    "\n",
    "\n",
    "Before we can analyze the history data, we want to add some information. For example, the initial dataset does not directly provide the trips (departures/arrivals) of the users but only the number of bikes or places available at a given time (per 5 minutes). To make an analysis of the frequentation or of the departure and arrival zones according to the time of the day or the week I calculated the departures and arrivals by 30min intervals.\n",
    "\n",
    "From the 30 minutes intervals we can for example infer the daily number.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a copy of our dataframe, to be able to return to the initial data if necessary\n",
    "df_sampled = df_bikes.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Calculation of the number of daily arrivals and departures\n",
    "\n",
    "\n",
    "We can make calculations directly by grouping the rows with the `groupby()` method.\n",
    "\n",
    "Which columns should be grouped together to be able to calculate the daily departures and arrivals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The 'transform' method allows to apply a calculation to the original dataframe (not grouped). \n",
    "## In our case, we want to perform a sum on the columns departure30min and arrival30min.\n",
    "\n",
    "## Complete the list of columns\n",
    "df_sampled[\"daily_departure\"] = df_sampled.groupby([****])['departure30min'].transform('sum')\n",
    "df_sampled[\"daily_arrival\"] = df_sampled.groupby([****])['arrival30min'].transform('sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display 15 rows randomly to see the result\n",
    "df_sampled.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Distinction between weekdays and weekends\n",
    "\n",
    "In order to analyze the data we want to be able to distinguish weekdays from weekend days, for this we need to prepare the data in order to identify the weekend days.\n",
    "\n",
    "1. We define a function that returns true when the date is a weekday and false when it is the weekend\n",
    "2. We apply this function on each row of our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The function weekDay, takes 3 parameters: the year, the month and the day\n",
    "def weekDay(year, month, day):\n",
    "    ## This method returns true if the date is a day of the week, false otherwise\n",
    "    ## Use the datetime() function and the weekday() method\n",
    "    ## https://docs.python.org/fr/3/library/datetime.html#datetime.datetime\n",
    "    \n",
    "    ****\n",
    "    \n",
    "\n",
    "## Vectorize the function in order to apply it efficiently (in terms of calculation time) on the dataframe\n",
    "isWeekDay = np.vectorize(weekDay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a new column from the result of the function applied on all the rows of the dataframe\n",
    "df_sampled['IsWeekday'] = isWeekDay(****)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display a sample of the dataframe\n",
    "df_sampled.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Saving the preprocessed dataset\n",
    "\n",
    "In order to be able to reuse the dataset without redoing all the processing we save it in a CSV file.\n",
    "\n",
    "Use the [to_csv()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html) method from Pandas library to save the preprocessed dataframe into a CSV file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Method 1 : Export the dataframe as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled.to_csv('data-bikes-2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Method 2 : Export the dataframe as a csv in a zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_opts = dict(method='zip', archive_name='data-bikes-2.csv')  \n",
    "df_sampled.to_csv('data-bikes-2.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c1a0333fc97fefd4ce94a1c8087d31dccee1782a1c25c68ad4f13412d9023b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
