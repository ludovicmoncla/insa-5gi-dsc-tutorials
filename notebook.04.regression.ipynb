{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![INSA](https://www.insa-lyon.fr/sites/all/themes/insa/logo.png)\n",
    "\n",
    "# GI-5-DSC - Data Science: Regression\n",
    "***\n",
    "\n",
    "The objective of this part of the tutorial is to continue the analysis of velo'v data and to experiment with artificial intelligence methods for data prediction using regression methods\n",
    "We will add a layer of weather data and use regression methods to predict velo'v availability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importation des libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import folium\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import geopandas\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "import sklearn.cluster\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting the data\n",
    "\n",
    "First, the dataset must be loaded. \n",
    "On the one hand the data set containing the locations of the stations and on the other hand the usage history. \n",
    "The second one has been modified during the previous session. In order not to have to redo all the processing you can retrieve the `data-bikes-2.zip` archive directly.\n",
    "\n",
    "\n",
    "All the data used in this tutorial is available on the [git repository](https://github.com/ludovicmoncla/insa-5gi-dsc-tutorials/tree/main/data) and on [Moodle](https://moodle.insa-lyon.fr/course/view.php?id=4628). \n",
    "\n",
    "\n",
    "* Download the datasets\n",
    "1. data-stations.zip\n",
    "2. data-bikes-3.zip\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Google Colab users : download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://perso.liris.cnrs.fr/lmoncla/GI-5-DSC/data-bikes-3.zip\n",
    "! wget https://perso.liris.cnrs.fr/lmoncla/GI-5-DSC/data-stations.zip\n",
    "\n",
    "path = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Loading the data\n",
    "\n",
    "As last time, to load the data you just have to use the method [read_csv()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#pandas.read_csv) from the `Pandas` library. \n",
    "It takes as a parameter the path of the file you want to load. This file can be of 2 formats, either directly a CSV file, or a ZIP file containing a CSV. In our case it is therefore unnecessary to unzip the previously downloaded archives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## We load the data from the stations into a dataframe\n",
    "df_stations = pd.read_csv('data-stations.zip')\n",
    "\n",
    "## We now load the dataframe with the history data\n",
    "df_bikes = pd.read_csv('data-bikes-3.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the first rows\n",
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the first rows\n",
    "df_bikes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the size in memory\n",
    "df_bikes['time'] = pd.to_datetime(df_bikes['time']) \n",
    "df_bikes[['year', 'daily_departure', 'daily_arrival']] = df_bikes[['year', 'daily_departure', 'daily_arrival']].astype('int16')\n",
    "df_bikes[['month','day','hour','minute', 'bikes', 'bike_stands', 'departure30min','arrival30min']] = df_bikes[['month','day','hour','minute', 'bikes', 'bike_stands', 'departure30min','arrival30min']].astype('int8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adding an additional data layer: the weather\n",
    "\n",
    "\n",
    "### 3.1 Loading the data\n",
    "\n",
    "The `data-weather-lyon.csv` file contains the weather data by hour for the year 2021 for Lyon.\n",
    "You can download this file from [Moodle](https://moodle.insa-lyon.fr/mod/folder/view.php?id=180291) or from the [git repository](https://github.com/ludovicmoncla/insa-5gi-dsc-tutorials/blob/main/data/data-weather-lyon.csv)\n",
    "\n",
    "\n",
    "The weather dataset corresponds to the `NASA/POWER CERES/MERRA2 Native Resolution Hourly Data` dataset retrieved through the NASA POWER Project API : https://power.larc.nasa.gov/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can download the file directly from the code (wget need to be installed)\n",
    "!wget https://github.com/ludovicmoncla/insa-5gi-dsc-tutorials/blob/main/data/data-weather-lyon.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data into a dataframe\n",
    "df_weather = pd.read_csv('data-weather-lyon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the first rows\n",
    "df_weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. First overview of the weather data\n",
    "\n",
    "The dataset contains the following columns:\n",
    "\n",
    "- WD10M           MERRA-2 Wind Direction at 10 Meters (Degrees) \n",
    "- T2M             MERRA-2 Temperature at 2 Meters (C) \n",
    "- RH2M            MERRA-2 Relative Humidity at 2 Meters (%) \n",
    "- QV2M            MERRA-2 Specific Humidity at 2 Meters (g/kg) \n",
    "- T2MDEW          MERRA-2 Dew/Frost Point at 2 Meters (C) \n",
    "- U10M            MERRA-2 Eastward Wind at 10 Meters (m/s) \n",
    "- PS              MERRA-2 Surface Pressure (kPa) \n",
    "- T2MWET          MERRA-2 Wet Bulb Temperature at 2 Meters (C) \n",
    "- WS10M           MERRA-2 Wind Speed at 10 Meters (m/s) \n",
    "- V10M            MERRA-2 Northward Wind at 10 Meters (m/s) \n",
    "- PRECTOTCORR     MERRA-2 Precipitation Corrected (mm/hour) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the method [drop()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) to delete the columns you don't want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We remove useless columns\n",
    "## We only want to keep the temperature, wind speed and rainfall columns.\n",
    "\n",
    "df_weather = *****\n",
    "\n",
    "df_weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the method [rename()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) to rename the remaining columns in order to be able to make a join with the vélo'v availability data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Renaming columns\n",
    "\n",
    "df_weather = *****\n",
    "df_weather.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the data to see their distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig,(ax1,ax2,ax3)= plt.subplots(nrows=3)\n",
    "fig.set_size_inches(12,20)\n",
    "\n",
    "monthAggregated = pd.DataFrame(*****).reset_index()\n",
    "monthSorted = monthAggregated.sort_values(by='temperature', ascending = False) \n",
    "sn.barplot(data=monthSorted, x = 'month', y = 'temperature', ax=ax1)\n",
    "ax1.set(xlabel='Month', ylabel='Average Temperature', title='Average Temperature by Month') \n",
    "\n",
    "monthAggregated = pd.DataFrame(*****).reset_index()\n",
    "monthSorted = monthAggregated.sort_values(by='precipitation', ascending = False) \n",
    "sn.barplot(data=monthSorted, x = 'month', y = 'precipitation', ax=ax2)\n",
    "ax2.set(xlabel='Month', ylabel='Average Precipitation', title='Average Precipitation by Month') \n",
    "\n",
    "monthAggregated = pd.DataFrame(*****).reset_index()\n",
    "monthSorted = monthAggregated.sort_values(by='vent', ascending = False) \n",
    "sn.barplot(data=monthSorted, x = 'month', y = 'vent', ax=ax3)\n",
    "ax3.set(xlabel='Month', ylabel='Average Wind Speed', title='Average Wind Speed by Month') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [merge()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) method to combine the velo'v availability data with the weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On ajoute les données météo dans le dataframe qui contient les données de disponibilité\n",
    "df_bikes = *****\n",
    "df_bikes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training of a prediction model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Preparation of data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We make a copy of our DataFrame, to be able to return to the initial data if necessary\n",
    "df_data = df_bikes.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the dataset into inputs and outputs for learning\n",
    "# the input correspond to the data provided to the model in order to learn. \n",
    "inputs = *****\n",
    "# outputs or labels correspond to the values that we want to predict and that the model must learn\n",
    "labels = *****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning methods can only use numerical variables. It is therefore necessary to transform categorical variables such as `id_velov` or `day_of_week` so that they can be used for learning. \n",
    "\n",
    "For that, we use the function [LabelEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) of the scikit-learn library. It allows to transform string values into numerical values by association. For example : Monday -> 0, Tuesday -> 1, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trabnsform categorical variables into numerical variables\n",
    "# Declare label encoders\n",
    "l1_encoder = LabelEncoder()\n",
    "l2_encoder = LabelEncoder()\n",
    "\n",
    "# We train the encoder to see all possible values\n",
    "l1_encoder.fit(df_stations.id_velov)\n",
    "\n",
    "# we transform the id_velov column\n",
    "inputs['id_velov'] = l1_encoder.transform(inputs['id_velov'])\n",
    "\n",
    "# We do the same for the day_of_week column\n",
    "l2_encoder.fit(inputs['day_of_week'])\n",
    "inputs['day_of_week'] = l2_encoder.transform(inputs['day_of_week'])\n",
    "\n",
    "# we check the results\n",
    "inputs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is ready and the variables are in the right format. Now we have to separate the dataset to use a part of the data for training and another part for evaluation. \n",
    "\n",
    "For this we use the method [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) which allows to separate the dataset in 2 random sets so that the training and evaluation datasets have the same characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We separate the dataset in 2: a training set and a test set\n",
    "x_train, x_test, y_train, y_test = *****\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Supervised learning\n",
    "\n",
    "All the supervised learning algorithms implemented in the Scikit-Learn library are available here: https://scikit-learn.org/stable/supervised_learning.html\n",
    "\n",
    "In our case, we want to predict continuous values as opposed to discrete values. We will therefore use regression models and not classification models.\n",
    "\n",
    "I propose you to test and compare 2 methods : [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) et [RandomForestRegressor](https://scikit-learn.org/0.15/modules/generated/sklearn.ensemble.RandomForestRegressor.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We declare a model of type RadomForestRegressor and we train it on the training set\n",
    "clf_lr = *****\n",
    "\n",
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the model to make the prediction on the test set\n",
    "predictions_lr = *****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We declare a model of type RadomForestRegressor and we train it on the training set\n",
    "clf_rf = *****\n",
    "*****\n",
    "\n",
    "# !! This cell can take several minutes to execute (between 5 and 10 min) !! #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the model to make the prediction on the test set\n",
    "predictions_rf = *****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Evaluation and comparison\n",
    "\n",
    "In order to evaluate our prediction models, we use the [MAE](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html) (Mean Absolute Error) measure: \n",
    "$$\\frac{\\sum_{i=1}^n \\left\\lvert \\hat{y}_i - y_i \\right\\rvert}{n}$$ \n",
    "which allows us to measure the absolute mean difference between the predictions and the values to be predicted. The lower this value is, the better our model is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The average error is calculated by comparing the predictions with the values that should be predicted\n",
    "\n",
    "mae_lr = *****\n",
    "\n",
    "print('linear regression mae : ', mae_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The average error is calculated by comparing the predictions with the values that should be predicted\n",
    "\n",
    "mae_rf = *****\n",
    "\n",
    "print('random forest regression mae : ', mae_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the average error is much lower for the random forest model! On average the difference between the prediction and the reality is 1. This seems rather satisfactory for our task. In the case of using this model in an application, the user would have to be warned that the prediction is on average correct to within 1 bike!\n",
    "\n",
    "We now want to display an evaluation in graphical form. Since the test set contains more than a million rows, we first isolate a sample of 100 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We group the values to be predicted and the predictions in the same table\n",
    "t = np.stack((y_test, predictions_rf, predictions_lr), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check the size of the table\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We display an overview of the values\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select a sample of 100 predictions to display the comparison\n",
    "\n",
    "idx = np.random.randint(t.shape[1], size=100)\n",
    "sample = t[:,idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.title(\"Comparison of the  predictions\")\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"# bikes available\")\n",
    "plt.plot(range(100), sample[0], color =\"green\", label=\"Truth\")\n",
    "plt.plot(range(100), sample[1], color =\"blue\", label=\"Random Forest\")\n",
    "plt.plot(range(100), sample[2], color =\"red\", label=\"Linear Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to display the same type of graph but instead of displaying the prediction we want to display the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_rf = abs(sample[0]-sample[1])\n",
    "err_lr = abs(sample[0]-sample[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.title(\"Comparison of the  predictions\")\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.plot(range(100), err_rf, color =\"blue\", label=\"Random Forest\")\n",
    "plt.plot(range(100), err_lr, color =\"red\", label=\"Linear Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Using the model\n",
    "\n",
    "Does the weather affect the prediction?\n",
    "\n",
    "To find out, we can make a prediction of availability at a station for a certain date by varying the weather and observe if there are differences between the predictions.\n",
    "\n",
    "\n",
    "The model takes as input 10 parameters:\n",
    "- id_velov\n",
    "- year\n",
    "- month\n",
    "- day\n",
    "- hour\n",
    "- minute\n",
    "- day_of_week\n",
    "- temperature\n",
    "- wind\n",
    "- precipitation\n",
    "\n",
    "We propose to make a prediction for next Saturday at 8am at the station 3094 (Gare Part-Dieu). \n",
    "We make the prediction for 2 different cases: good and bad weather\n",
    "1. Good weather : temperature = 20, wind = 1, precipitation = 0\n",
    "2. Bad weather : temperature = 8, wind = 3, precipitation = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# encode the name of the station\n",
    "st = *****\n",
    "\n",
    "# encode the name of the day\n",
    "dn = *****\n",
    "\n",
    "# list of input data\n",
    "d_nice = *****\n",
    "d_bad = *****\n",
    "\n",
    "# make the prediction\n",
    "p_nice = *****\n",
    "p_bad = *****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_nice)\n",
    "print(p_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the weather does have an impact on the prediction. There will be more bikes available in bad weather than in good weather!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice\n",
    "\n",
    "Using the code from the previous sessions, propose a map of the availability of vélo'v for all the stations for tomorrow at 8 am."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('stanza-lexicoscope-py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "68d5f9281eab57a7f4901cb150f4c691b1d08935474a18f188e0e3e8f8f412b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
